{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "from ray.tune import Analysis\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "__file__ = globals()['_dh'][0]\n",
    "\n",
    "# get an absolute path to the directory that contains parent files\n",
    "project_dir = __file__ = globals()['_dh'][0]\n",
    "sys.path.append(os.path.normpath(os.path.join(project_dir, '..', '..')))\n",
    "\n",
    "from experiments.utils.constants import TRAIN_RESULTS_PATH\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = 1\n",
    "total_timesteps = 100000\n",
    "\n",
    "\n",
    "evaluation_infos = {\n",
    "    # 0 : {\n",
    "    #     'cluster_id': 0,\n",
    "    #     'legend': 'PG',\n",
    "    #     'experiment_id': 4,\n",
    "    #     'algorithm': 'PG',\n",
    "    #     'workload_id': 0,\n",
    "    #     'trial': 'PG_SimSchedulerEnv_92f3d_00000_0_target_utilization=[0, 0]_2022-11-16_15-30-17'},\n",
    "    # 1: {\n",
    "    #     'cluster_id': 0,\n",
    "    #     'legend': 'DQN',\n",
    "    #     'experiment_id': 3,\n",
    "    #     'algorithm': 'DQN',\n",
    "    #     'workload_id': 0,\n",
    "    #     'trial': 'DQN_SimSchedulerEnv_70a79_00000_0_target_utilization=[0, 0]_2022-11-15_16-06-18'},\n",
    "    2: {\n",
    "        'cluster_id': 0,\n",
    "        'legend': 'PG',\n",
    "        'experiment_id': 0,\n",
    "        'algorithm': 'PG',\n",
    "        'workload_id': 0,\n",
    "        'trial': 'PG_SimSchedulerEnv_caf24_00000_0_episode_length=10_2022-11-14_00-17-58'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiments(cluster_id, legend, experiment_id, workload_id, algorithm, trial):\n",
    "    experiment_folder_path = os.path.join(\n",
    "        TRAIN_RESULTS_PATH,\n",
    "        \"series\",      str(series),\n",
    "        \"envs\",        'sim-scheduler',\n",
    "        \"clusters\",    str(cluster_id),\n",
    "        \"workloads\",   str(workload_id),\n",
    "        \"experiments\", str(experiment_id),\n",
    "        str(algorithm), trial)\n",
    "    analysis = Analysis(experiment_folder_path)\n",
    "    df = analysis.trial_dataframes[experiment_folder_path]\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            experiment_folder_path, 'result.json'), 'r') as f:\n",
    "        result = f.read().split('\\n')\n",
    "    # episodes_reward = pd.DataFrame({'timestep':[], 'reward':[]})\n",
    "    experiment_json = {}\n",
    "    # TODO also load the df and do from scrath with the goal of replicating the tensorboard results\n",
    "    for line, iteration in enumerate(result):\n",
    "        if iteration != '':\n",
    "            experiment_json[line] = json.loads(iteration)\n",
    "    # replace rewards with new ones\n",
    "    selected_stats = [\n",
    "        'episode_reward_mean', 'episodes_this_iter',\n",
    "        'timesteps_total',\n",
    "        'custom_metrics/scheduling_timestep_avg_mean',\n",
    "        'custom_metrics/scheduling_success_avg_mean',\n",
    "        'custom_metrics/num_consolidated_avg_mean',\n",
    "        'custom_metrics/num_overloaded_avg_mean',\n",
    "        'custom_metrics/time_mean',\n",
    "        'custom_metrics/timestep_episode_mean',\n",
    "        'custom_metrics/reward_u_mean',\n",
    "        'custom_metrics/reward_c_mean',\n",
    "        'custom_metrics/reward_v_mean',\n",
    "        'custom_metrics/reward_g_mean',\n",
    "        'custom_metrics/reward_p_mean']\n",
    "    experiment_df = df[selected_stats]\n",
    "    return legend, cluster_id, experiment_df, experiment_json\n",
    "\n",
    "\n",
    "experiments_df = {}\n",
    "experiments_json = {}\n",
    "# episodes_rewards = []\n",
    "for evaluation_id, evaluation_info in evaluation_infos.items():\n",
    "    legend, cluster_id, experiment_df, experiment_json = load_experiments(**evaluation_info)\n",
    "    experiments_json[evaluation_id] = experiment_json\n",
    "    experiments_df[evaluation_id] = experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.254704\n",
       "1       1.194489\n",
       "2       1.265476\n",
       "3       1.247487\n",
       "4       1.212903\n",
       "          ...   \n",
       "1477    2.064516\n",
       "1478    2.063492\n",
       "1479    2.066138\n",
       "1480    2.072581\n",
       "1481    2.064516\n",
       "Name: custom_metrics/num_consolidated_avg_mean, Length: 1482, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id = 2\n",
    "metric = 'custom_metrics/num_consolidated_avg_mean'\n",
    "experiments_df[experiment_id][metric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experime\n",
    "experiment_ids = key_config_df[\n",
    "    key_config_df['model_variant'] == model_variant]['experiment_id'].tolist()\n",
    "metadata_columns = ['model_variant', 'max_batch_size', 'cpu_request', 'load']\n",
    "results_columns = ['client_to_model_latencies_avg', 'model_latencies_avg', 'model_to_client_latencies_avg']\n",
    "output = loader1.table_maker(\n",
    "    experiment_ids=experiment_ids,\n",
    "    metadata_columns=metadata_columns,\n",
    "    results_columns=results_columns)\n",
    "display(output)\n",
    "ax = output.plot.bar(x='load', y=['client_to_model_latencies_avg', 'model_latencies_avg', 'model_to_client_latencies_avg'])\n",
    "ax.set_xlabel(\"Load\")\n",
    "ax.set_ylabel(\"Latency (seconds)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e2adeafa4d8ea8b56364dfeb24ca99374007784e59d4b40ac8f07b769210312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
